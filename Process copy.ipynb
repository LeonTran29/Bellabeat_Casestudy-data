{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92fb35b",
   "metadata": {},
   "source": [
    "This text describes a data analysis project aimed at guiding Bellabeat's marketing strategy. The goal is to use smart device data to find key insights. The process involves a clear, five-step approach:\n",
    "\n",
    "Ask: Define the main business problem and the questions to be answered.\n",
    "Prepare: Examine the dataset to understand its context and quality.\n",
    "Process: Clean and transform the data for analysis.\n",
    "Analyze: Conduct an exploratory data analysis (EDA) to find patterns in user behavior.\n",
    "Share: Summarize the findings and offer actionable marketing recommendations.\n",
    "\n",
    "The ultimate purpose is to provide Bellabeat with a data-driven basis for making better marketing choices, with a special focus on understanding user activity, segmenting customers, and boosting engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b55296",
   "metadata": {},
   "source": [
    "Bellabeat, a company that makes smart wellness devices for women, wants to better understand the people who use smart devices in general. By analyzing data from non-Bellabeat smart devices, they hope to find new opportunities and make smarter marketing decisions.\n",
    "\n",
    "This analysis will look at user behavior patterns, including physical activity, sedentary habits, and how people engage with different features. The insights from this study will help Bellabeat's marketing team to:\n",
    "\n",
    "- Target the right customers.\n",
    "- Improve promotional strategies to increase user engagement.\n",
    "- Guide product development with real-world usage data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8ab6d",
   "metadata": {},
   "source": [
    "ASK\n",
    "\n",
    "The right questions is importatnt to ensure the analysis staying aligned with business needs. The general view with problem statements, which pave a direction and goals of the analysis, is defined.\n",
    "\n",
    "The key is defined as follow:\n",
    "1. What:\n",
    "    - Problem: Understand general consumer wellness habits as captured by smart device data to identify growth opportunities and refine product strategy.\n",
    "    - Goal: Extract actionable insights from analyzed consumer patterns to inform marketing strategy and potential product enhancements, which driving user acquisitions and engagement for Bellabeat and support Bellabeat's ambition to become a larger player in the global smart device market.\n",
    "2. Who:\n",
    "    - Who is the stake holder: Urška Sršen (CCO), Sando Mur (key member), marketing team.\n",
    "    - Who is the target: women customer in smart device market.\n",
    "3. When:\n",
    "    - The data is collected between March 12, 2016 - May 12, 2016. This period is several year old.\n",
    "4. Where:\n",
    "    - Data source: publicly available \"FitBit Fitness Tracker Data\" from Kaggle.\n",
    "    - Geographic Context: unknown, it is considered as applying insights to global market.\n",
    "5. How:\n",
    "    - Data Acquisition & Cleaning: Source and preparing the selected FitBit dataset for analysis.\n",
    "    - EDA: Performing descriptive statistic to understand the distribution and summary of key metrics (activity, sleep, stes, calories, etc.)\n",
    "    - Pattern Identification: Understand trends, correlation between health metrics, and common behavioral patterns.\n",
    "    - Segmentation: Segmenting users based on their activity or sleep pattern.\n",
    "    - Visualization: Creating charts and dashboards to communicate complex findings.\n",
    "    - Insight Generation & Recommendation: Translate data-driven insight into practical, actionable reccommendation specifically for Bellabeat's ambition, attract and retain their target audience.\n",
    "6. Why:\n",
    "    - Gain a Competitive Edge: Understand broader market trends and user preferences beyond their current customer base, identifying what drives engagement in the smart health sector.\n",
    "    - Unlock New Growth Opportunities: Pinpoint untapped market segments, potential feature gaps, or areas for product innovation that resonate with prevalent consumer habits.\n",
    "    - Optimize Marketing & Product Strategy: Align Bellabeat's outreach, messaging, and app features more precisely with proven user behaviors and preferences, thereby increasing the effectiveness of marketing campaigns and the value proposition of the Bellabeat app.\n",
    "    - Inform Data-Driven Decisions: Provide key stakeholders with the necessary insights to make informed strategic decisions regarding product development, marketing spend, and overall market expansion, ensuring Bellabeat's offerings are relevant and highly appealing to health-conscious women"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20113b",
   "metadata": {},
   "source": [
    "PREPARE\n",
    "\n",
    "1. Data Collection\n",
    "The dataset is publicly available from Kaggle with licensed of CC0: Public Domain. This allows the data to be used for various purposes, even for commercial purposes without the permission of the author.\n",
    "\n",
    "2. Data Understanding\n",
    "Overview of dataset directory path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "# --- Import utils  ---\n",
    "from utils import check_missing_values, check_information, check_duplicates, display_duplicates\n",
    "\n",
    "# --- Define your data folders ---\n",
    "# Define source path\n",
    "source_path = 'Dataset'\n",
    "\n",
    "dir_count = 1\n",
    "file_count = 1\n",
    "for dirpath, dirnames, filenames in os.walk(source_path):\n",
    "    print(f\"{dir_count}. Directory: {dirpath}\")\n",
    "    dir_count += 1\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.csv'):\n",
    "            print(f\"  {file_count}. File: {filename}\")\n",
    "            file_count += 1\n",
    "    file_count = 1  # Reset file count for the next directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eece6f3",
   "metadata": {},
   "source": [
    "Several data in this project is stored in two separate CSV file. In general, the data is set into 2 groups:\n",
    "    - Group 1: from March 12th, 2016 to April 11th, 2016.\n",
    "    - Group 2: from April 12th, 2016 to May 12th, 2016.\n",
    "\n",
    "The goal is to understanding the consumer behavior as a whole, so those two dataset will be merged into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import DaiyActivity ---\n",
    "\n",
    "dir1 = 'Dataset/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16'\n",
    "dir2 = 'Dataset/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16'\n",
    "\n",
    "dailyActivity1 = pd.read_csv(os.path.join(dir1, 'dailyActivity_merged.csv'))\n",
    "dailyActivity2 = pd.read_csv(os.path.join(dir2, 'dailyActivity_merged.csv'))\n",
    "\n",
    "df = pd.concat([dailyActivity1, dailyActivity2], ignore_index=True)\n",
    "print('The whole dataset has {} rows and {} columns.'.format(df.shape[0],df.shape[1]))\n",
    "print('The dataset contains the following columns:')\n",
    "print(df.columns.tolist())\n",
    "display(df.head(10))\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"Data types of each column:\")\n",
    "display(df.info())\n",
    "# Header of the DataFrame\n",
    "print(\"\\nSummary statistics of the DataFrame:\")\n",
    "display(df.head())\n",
    "# Total user count\n",
    "total_users = df['Id'].nunique()\n",
    "print(f\"\\nTotal number of unique users: {total_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import heartrate_seconds_merged ---\n",
    "heartRate1 = pd.read_csv(os.path.join(dir1, 'heartrate_seconds_merged.csv'))\n",
    "heartRate2 = pd.read_csv(os.path.join(dir2, 'heartrate_seconds_merged.csv'))\n",
    "\n",
    "heartRate1['datetime_full'] = pd.to_datetime(heartRate1['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "heartRate1['Date'] = heartRate1['datetime_full'].dt.date\n",
    "heartRate1['Time'] = heartRate1['datetime_full'].dt.time\n",
    "heartRate1.drop(columns=['datetime_full'], inplace=True)\n",
    "#print(heartRate1.columns.tolist())\n",
    "new_columns = ['Id', 'Date','Time', 'Value']\n",
    "heartRate1 = heartRate1.reindex(columns=new_columns)\n",
    "\n",
    "heartRate2['datetime_full'] = pd.to_datetime(heartRate2['Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "heartRate2['Date'] = heartRate2['datetime_full'].dt.date\n",
    "heartRate2['Time'] = heartRate2['datetime_full'].dt.time\n",
    "heartRate2.drop(columns=['datetime_full'], inplace=True)\n",
    "#print(heartRate1.columns.tolist())\n",
    "new_columns = ['Id', 'Date','Time', 'Value']\n",
    "heartRate2 = heartRate2.reindex(columns=new_columns)\n",
    "\n",
    "display(heartRate1.head(10))\n",
    "display(heartRate2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import minuteSleep_merged ---\n",
    "minuteSleep1 = pd.read_csv(os.path.join(dir1, 'minuteSleep_merged.csv'))\n",
    "minuteSleep2 = pd.read_csv(os.path.join(dir2, 'minuteSleep_merged.csv'))\n",
    "\n",
    "minuteSleep1['datetime_full'] = pd.to_datetime(minuteSleep1['date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "minuteSleep1['Date'] = minuteSleep1['datetime_full'].dt.date\n",
    "minuteSleep1['Time'] = minuteSleep1['datetime_full'].dt.time\n",
    "minuteSleep1.drop(columns=['date','datetime_full'], inplace=True)\n",
    "#print(minuteSleep1.columns.tolist())\n",
    "new_columns = ['Id', 'Date','Time', 'value', 'logId']\n",
    "minuteSleep1 = minuteSleep1.reindex(columns=new_columns)\n",
    "\n",
    "minuteSleep2['datetime_full'] = pd.to_datetime(minuteSleep2['date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "minuteSleep2['Date'] = minuteSleep2['datetime_full'].dt.date\n",
    "minuteSleep2['Time'] = minuteSleep2['datetime_full'].dt.time\n",
    "minuteSleep2.drop(columns=['date','datetime_full'], inplace=True)\n",
    "new_columns = ['Id', 'Date','Time', 'value', 'logId']\n",
    "minuteSleep2 = minuteSleep2.reindex(columns=new_columns)\n",
    "\n",
    "display(minuteSleep1)\n",
    "display(minuteSleep2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2764dc8",
   "metadata": {},
   "source": [
    "3. Initial Data Exploration\n",
    "The datasets contains daily activity metrics which are recorded by a smart device. The metrics and their descriptions are as following:\n",
    "\n",
    "A. In dailyActivity:\n",
    "- Id:\tUnique identifier for each user\n",
    "- ActivityDate:\tDate&Time when the activity was recorded\n",
    "- TotalSteps:\tTotal number of steps taken by the user in a day\n",
    "- TotalDistance:\tTotal distance covered in a day (combines both tracked and manually logged activities)\n",
    "- TrackerDistance:\tDistance automatically tracked by the device in kilometer (includes all activity levels)\n",
    "- LoggedActivitiesDistance:\tDistance manually entered by the user (in kilometer)\n",
    "- VeryActiveDistance:\tDistance covered during high-intensity (very active) activities\n",
    "- ModeratelyActiveDistance:\tDistance covered during moderate-intensity activities\n",
    "- LightActiveDistance:\tDistance covered during low-intensity (light active) activities\n",
    "- SedentaryActiveDistance:\tDistance recorded during sedentary state (usually very low or zero)\n",
    "- VeryActiveMinutes:\tTime spent in very active activities (in minutes)\n",
    "- FairlyActiveMinutes:\tTime spent in moderately active activities (in minutes)\n",
    "- LightlyActiveMinutes:\tTime spent in light activities (in minutes)\n",
    "- SedentaryMinutes:\tTime spent being sedentary (e.g., sitting or lying down)\n",
    "- Calories:\tTotal calories burned in a day\n",
    "\n",
    "B. In heartRate:\n",
    "- Id:\tUnique identifier for each user\n",
    "- Time:\tDate&Time when the activity was recorded\n",
    "- Value: Value of recorded heart rate\n",
    "\n",
    "C. In minuteSleep:\n",
    "- Id:\tUnique identifier for each user\n",
    "- Date:\tDate&Time when the activity was recorded\n",
    "- Value: Sleep stage according to the manual of the device\n",
    "- LogId: Unique identifier for each specific sleep session\n",
    "\n",
    "4. Data Assessment & Cleaning\n",
    "4.1. Missing values\n",
    "    - If the records is missing, additional data should be collected to ensure the sample was representative.\n",
    "    - If the missing can't be collected, data in the same column should be considered to adjust and fill the missing (mean, median, mode, or a specific value of 0).\n",
    "    - If nothing can be done, business objective should be reconsidered to align the data.\n",
    "4.2. Duplicate records\n",
    "    - If the dupplicate records are found, remove the duplicate rows, which could lead to the overpresentation of data points.\n",
    "4.3. Incorrect Data Types\n",
    "    - If a data types is error, it should be converted into the correct data type.\n",
    "4.4. Inconsistent Data Entry\n",
    "    - If the incorrect is found, they should be corrected if possible (e.g., formatting issues, unrealistic values)\n",
    "    - If the incorrect can't be fixed, they should be noted or even excluded from the dataset. But ensure the remaining dataset still meet the sample size requirements for analysis.\n",
    "4.5. Column Name Issues\n",
    "    - If the column name is hard to understand, it should be considered to be rename for clarity and consistency.\n",
    "4.6. Outlier\n",
    "    - As extreme value can heavily skew average, standard deviations, and statistical model, it can be a result of entry mistake. If the extreme value is found, it should be investigated further, or eliminated before the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a767688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of each column\n",
    "print(\"\\nSummary statistics of the DataFrame:\")\n",
    "display(df.describe().transpose())\n",
    "# Display summary statistics of the DataFrame\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaca65b",
   "metadata": {},
   "source": [
    "Data Context & Sampling\n",
    "Link: https://www.demandsage.com/smartwatch-statistics/\n",
    "\n",
    "The representative of the dataset should be considered before analysis. As Bellabeat's products are aimed to sell globally, the relevant customer should be included in the smartwatch users worldwide.\n",
    "\n",
    "According to Demandsage, there are 562.86 million smartwatch users globally in 2025, and 40% of them is women, around 225.14 million female users.\n",
    "\n",
    "With that population, the smallest sample size, which should be enough to make analysis confidently with a 95% confidence level and a 5% margin of error, is 385 samples. It's far larger than the current sample size, 35 users, with a 44.59% confidence level (estimated a 5% margin of error). This sample size of 35 users is not sufficient to respresent the population, which should be noted during interpreting results.\n",
    "\n",
    "To improve the sample size and enrich the insight quality, the sample size must be increased to sufficient level. It can be done by asking for data from the provider, but it requires authorization, statement of intended use, and organization representation.\n",
    "\n",
    "The analysis will be proceed with the available dataset as it's sufficient for the primary objective in this case - to practice data analysis in the real-world case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da375c9",
   "metadata": {},
   "source": [
    "PROCESS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915798c",
   "metadata": {},
   "source": [
    "???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a26572",
   "metadata": {},
   "source": [
    "Daily Activity Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChecking for missing values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_missing_values(df))\n",
    "\n",
    "print(\"\\nChecking for duplication values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_duplicates(df))\n",
    "\n",
    "print(\"\\nChecking DataFrame information:\")\n",
    "print(\"-------------------------------------------\")\n",
    "check_information(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['ActivityDate'] =  pd.to_datetime(df['ActivityDate'], format='%m/%d/%Y')\n",
    "display(df.info())\n",
    "display(df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dff0ae",
   "metadata": {},
   "source": [
    "Heart Rate Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartRate = pd.concat([heartRate1, heartRate2], ignore_index=True)\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_missing_values(heartRate))\n",
    "\n",
    "print(\"\\nChecking for duplication values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_duplicates(heartRate))\n",
    "\n",
    "print(\"\\nDisplaying duplicates in heartRate DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(display_duplicates(heartRate))\n",
    "\n",
    "print(\"\\nChecking DataFrame information:\")\n",
    "print(\"-------------------------------------------\")\n",
    "check_information(heartRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartRate.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae33748",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartRate['Date'] =  pd.to_datetime(heartRate['Date'], format='%m/%d/%Y')\n",
    "display(heartRate.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bb05c",
   "metadata": {},
   "source": [
    "After checking the specific zone of duplication, I found no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ecc8dd",
   "metadata": {},
   "source": [
    "Sleep Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa60831",
   "metadata": {},
   "outputs": [],
   "source": [
    "minuteSleep = pd.concat([minuteSleep1, minuteSleep2], ignore_index=True)\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_missing_values(minuteSleep))\n",
    "\n",
    "print(\"\\nChecking for duplication values:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_duplicates(minuteSleep))\n",
    "\n",
    "print(\"\\nDisplaying duplicates in minuteSleep DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(display_duplicates(minuteSleep))\n",
    "\n",
    "print(\"\\nChecking DataFrame information:\")\n",
    "print(\"-------------------------------------------\")\n",
    "check_information(minuteSleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minuteSleep.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "minuteSleep['Date'] =  pd.to_datetime(minuteSleep['Date'], format='%m/%d/%Y')\n",
    "display(minuteSleep.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe32de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate heartRate_merged ---\n",
    "## Why choose the gap of 15 seconds?\n",
    "heartRate['Seconds'] = pd.to_timedelta(heartRate['Time'].astype(str)).dt.total_seconds()\n",
    "display(heartRate.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heartRate['TimeDiff'] = heartRate.groupby(['Id','Date'])['Seconds'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heartRate.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7986ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=heartRate, x='TimeDiff', kde=True)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Distribution of Time Differences', fontsize=16)\n",
    "plt.xlabel('Time Difference (Minutes)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.ylim(0, 2100000)  # Adjust y-axis limit for better visibility\n",
    "plt.show()\n",
    "\n",
    "most_common_time_diff = heartRate['TimeDiff'].mode()\n",
    "print(f\"Most common time difference: {most_common_time_diff}\")\n",
    "display(most_common_time_diff)\n",
    "time_diff_counts = heartRate['TimeDiff'].value_counts()\n",
    "print(f\"\\nCounts of each time difference:\\n{time_diff_counts}\")\n",
    "display(time_diff_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(heartRate.head(10))\n",
    "wear_tracking_agg = heartRate.groupby(['Id', 'Date']).apply(\n",
    "    lambda x: x.loc[x['TimeDiff'] <= 15, 'TimeDiff'].sum(),\n",
    "    include_groups=False\n",
    ").reset_index(name = 'Total Wear Time (Seconds)')\n",
    "\n",
    "wear_tracking_agg['Total Wear Time (Minutes)'] = wear_tracking_agg['Total Wear Time (Seconds)'] / 60\n",
    "wear_tracking_agg['Total Wear Time (Hours)'] = pd.to_timedelta(wear_tracking_agg['Total Wear Time (Seconds)'], unit='s').apply(lambda x: str(x).split()[-1])\n",
    "print(\"\\nAggregated wear tracking DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(wear_tracking_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate heartRate_merged ---\n",
    "heartRate_agg = heartRate.groupby(['Id', 'Date']).agg(\n",
    "    min_heart_rate=('Value', 'min'),\n",
    "    max_heart_rate=('Value', 'max'),\n",
    "    mean_heart_rate=('Value', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nAggregated heartRate DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(heartRate_agg)\n",
    "\n",
    "check_duplicates(heartRate_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa39977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate minuteSleep_merged ---\n",
    "minuteSleep_agg = minuteSleep.groupby(['Id','Date']).agg(\n",
    "    Sleep_from_time = ('Time','min'),\n",
    "    Sleep_to_time = ('Time','max'),\n",
    "    total_sleep = ('value','count'),\n",
    "    logId = ('logId','first'),\n",
    "    Minutes_Asleep = ('value', lambda x: (x == 1).sum()),\n",
    "    Minutes_Restless = ('value', lambda x: (x == 2).sum()),\n",
    "    Minutes_Awake = ('value', lambda x: (x == 3).sum())\n",
    ").reset_index()\n",
    "\n",
    "display(minuteSleep_agg)\n",
    "\n",
    "# Combine the two aggregated DataFrames\n",
    "display(minuteSleep_agg.info())\n",
    "\n",
    "check_duplicates(minuteSleep_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDisplaying the first 5 rows of each DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Daily Activity DataFrame:\")\n",
    "display(df.head(5))\n",
    "print(\"\\nMinute Sleep Aggregated DataFrame:\")\n",
    "display(minuteSleep_agg.head(5))\n",
    "print(\"\\nHeart Rate Aggregated DataFrame:\")\n",
    "display(heartRate_agg.head(5))\n",
    "print(\"\\nWear Tracking Aggregated DataFrame:\")\n",
    "display(wear_tracking_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c176cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = pd.merge(df, minuteSleep_agg, left_on=['Id', 'ActivityDate'], right_on=['Id', 'Date'], how='left')\n",
    "merge2 = pd.merge(merge1, heartRate_agg, left_on=['Id', 'ActivityDate'], right_on=['Id', 'Date'], how='left')\n",
    "df_merge = pd.merge(merge2, wear_tracking_agg, left_on=['Id', 'ActivityDate'], right_on=['Id', 'Date'], how='left')\n",
    "df_merge.drop(columns=['Date_x', 'Date_y','Date'], inplace=True)\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "check_information(df_merge)\n",
    "\n",
    "print(\"Checking for missing values in the merged DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(check_missing_values(df_merge))\n",
    "\n",
    "print(\"Checking for duplication values in the merged DataFrame:\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(check_duplicates(df_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the merged DataFrame\n",
    "print(\"\\nMissing values of sleep records in the merged DataFrame:\")\n",
    "missing_sleep_records = df_merge['logId'].isnull().sum()\n",
    "print(f\"Number of missing sleep records: {missing_sleep_records}\")\n",
    "missing_heart_rate_records = df_merge['min_heart_rate'].isnull().sum()\n",
    "print(f\"Number of missing heart rate records: {missing_heart_rate_records}\")\n",
    "missing_wear_tracking_records = df_merge['Total Wear Time (Seconds)'].isnull().sum()\n",
    "print(f\"Number of missing wear tracking records: {missing_wear_tracking_records}\")\n",
    "total_records = df_merge.shape[0]\n",
    "print(f\"Total records in the merged DataFrame: {total_records}\")\n",
    "\n",
    "data = {\n",
    "    'Total records': total_records,\n",
    "    'Missing sleep records': missing_sleep_records,\n",
    "    'Missing heart rate records': missing_heart_rate_records,\n",
    "    'Missing wear tracking records': missing_wear_tracking_records\n",
    "}\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "missing_records = pd.DataFrame(data.items(), columns=['Columns', 'Count']).reset_index(drop=True)\n",
    "missing_records['Percentage'] = (missing_records['Count'] / total_records) * 100\n",
    "display(missing_records)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(data = missing_records, x ='Columns', y='Count', ax=ax, palette=colors)\n",
    "for index, row in missing_records.iterrows():\n",
    "    ax.text(\n",
    "        index,\n",
    "        row['Count'] + 5,\n",
    "        f\"{row['Count']}\"+f\" ({row['Percentage']:.2f}%)\",\n",
    "        color='black',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=12\n",
    "    )\n",
    "ax.set_title('Missing Records in Merged DataFrame', fontsize=16)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xlabel('Record Type', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merge.to_csv('df_merge.csv', index=False)\n",
    "#print(\"\\nMerged DataFrame saved to 'df_merge.csv'.\")\n",
    "\n",
    "display(df_merge['TotalSteps'].sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb646a33",
   "metadata": {},
   "source": [
    "The tracking defines that in total records of 62 days, people dont usually wearing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Macenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
